{
  "episode_number": "167",
  "episode_title": "AI SERIES: 12 - The Evolution of AI: Rebrands, Capabilities, and Future Trends with Peter Correia",
  "guest_name": "Peter Correia",
  "business_name": "Bulk Upload",
  "transcript": "ï»¿Nikolas Hulewsky (00:04.398)\r\nSo yeah.\r\n\r\n\r\nSo we have about 30 minutes. right. Peter, I am excited to learn about AI from you today. Like you have no idea, I can't sleep the last week. It's just been drinking from a fire hydrant. So you have a really cool background and experience. Quickly tell us what do you do and what's your experience with AI?\r\n\r\n\r\nPeter Correia (00:13.039)\r\nNever.\r\n\r\n\r\nPeter Correia (00:23.65)\r\nYeah.\r\n\r\n\r\nPeter Correia (00:33.922)\r\nYep, definitely. So in the consulting space, my experience with AI dates back to about 10 years ago. I started getting an interest in deep learning. Went from the computer vision side to understand how deep learning models worked. And from there, with everything that came about with ChatGPT, using neural networks to help with more text-based processing, I fell back into love with AI and have started to just be tinkering ever since.\r\n\r\n\r\nNikolas Hulewsky (01:01.004)\r\nSo you were into AI before it was sexy.\r\n\r\n\r\nPeter Correia (01:03.628)\r\nRight, exactly. Yeah, whenever it's just...\r\n\r\n\r\nNikolas Hulewsky (01:05.514)\r\nOr better said, you were into AI when they called it machine learning.\r\n\r\n\r\nPeter Correia (01:10.09)\r\nRight, yeah, exactly.\r\n\r\n\r\nNikolas Hulewsky (01:12.278)\r\nIt is pretty funny to see some of these rebrands of like algorithm is now AI, machine learning is now AI, robotic process automation is now agents. mean, agents will be different than RPA, but right now it's like, that's essentially what agents are. So the rebrand has been amazing. Cool. And so you were telling me before a call.\r\n\r\n\r\nPeter Correia (01:20.291)\r\nRight.\r\n\r\n\r\nPeter Correia (01:25.038)\r\nYeah.\r\n\r\n\r\nPeter Correia (01:32.266)\r\nRight. Yeah, yeah, it's all the same thing. Exactly.\r\n\r\n\r\nNikolas Hulewsky (01:41.26)\r\nYou've done some really cool things in the tech space. You've obviously spun up your own consulting agency where you are helping people architect. What's the best word for it? Data flows, data architecture.\r\n\r\n\r\nPeter Correia (01:52.268)\r\nYeah, helping figure out how to connect their different systems to different data so they can help with different marketing segmentation activities.\r\n\r\n\r\nNikolas Hulewsky (02:00.225)\r\nYeah, but you've also built a really cool software that takes Zillow data and allows investors to process that more quickly and make better decisions. Did you make that using AI or was that just software you developed before you really got heavy into it? It was.\r\n\r\n\r\nPeter Correia (02:17.87)\r\nYeah, it was. Yeah, so I did it. It was probably just one by GPT-3, maybe GPT-4 was just coming out and I was able to sort of quickly spin that up. I was on paternity leave, so I had some time and I was able to connect with someone online who was an influencer, could kind of help with some distribution and just kind of spun it up real quick. I had some time to the Stripe API and sort of off to the races after that.\r\n\r\n\r\nNikolas Hulewsky (02:40.917)\r\nAnd so what was that business? know you said you own several rental properties, but what is this business in particular, software serving?\r\n\r\n\r\nPeter Correia (02:46.542)\r\nSo right now, it initially was serving more the real estate investor community of putting in addresses and getting back some of those zillometric data in more of a bulk upload process. Right now, figure that a lot of folks on the direct mail side have a use for some of this data, being able to segment out different property types, different home values, and then can provide certain offers based on those specific home type values.\r\n\r\n\r\nNikolas Hulewsky (02:52.461)\r\nMm-hmm.\r\n\r\n\r\nNikolas Hulewsky (03:13.709)\r\nand what's it called?\r\n\r\n\r\nPeter Correia (03:14.958)\r\nIt's called right now, it's bulk upload.info.\r\n\r\n\r\nNikolas Hulewsky (03:18.465)\r\nbulkupload.info. Dude, those are the best names because there's nothing worse than trying to remember, what's that website that does bulk uploads? it's philanganaing.com. You're like, that's a name no one's ever heard of. So I love that. Okay, cool. So what I'm most curious about is how are you utilized? Am I recording right now, by the way?\r\n\r\n\r\nPeter Correia (03:27.906)\r\nBye.\r\n\r\n\r\nPeter Correia (03:46.766)\r\nSounds so, I thought I sounded a little, yeah, I think so.\r\n\r\n\r\nNikolas Hulewsky (03:48.971)\r\nIt does say recording. OK, my button got weird. OK.\r\n\r\n\r\nthe question I always ask, which models are you using and what are you using them for?\r\n\r\n\r\nPeter Correia (04:00.94)\r\nYeah, so I really feel like I use probably all the models that are really popular at the moment. know, Claude, I think is great. And we'll kind of talk a little bit more about Claude. think ChatGPT, O1 Pro, I it really gets better at being sort of so expensive. I think if you are sort of using it, I feel I am. It's sort of like it justifies the cost right now. It still is something that when I run into a really hard problem, it will just sort of just\r\n\r\n\r\nNikolas Hulewsky (04:24.653)\r\nMm-hmm.\r\n\r\n\r\nPeter Correia (04:30.318)\r\nthrow it at it. a lot of times, it'll just help get me started going down the right direction. And then as well as Grok, I wasn't sort of surprising that to sort of come out and be as useful as it was. I don't know if it gets as deep into some of the different topics, but being able to use it for pretty low cost, it's been good as well.\r\n\r\n\r\nNikolas Hulewsky (04:51.287)\r\nYeah, it's been, it's been surprising to me to see how many people are saying they're using Grok already. Like it's pretty quickly become like a tier one model. And I don't know about performance. I mean, to me it's fantastic, but as usage it's become like this tier one model that's interesting. So you're using Claude for coding, ChatGPT for brainstorming ideation problem solving.\r\n\r\n\r\nPeter Correia (04:57.558)\r\nRight.\r\n\r\n\r\nPeter Correia (05:17.728)\r\nYeah, I think that's probably a good way to put it. I don't use Claude as much as that ideation partner. A lot of it's using chat GPT. 4.5 has been interesting.\r\n\r\n\r\nNikolas Hulewsky (05:30.029)\r\nI have a question, sorry, because I'm very stupid. One of the things I was trying to figure out this weekend is what's the difference between all the models? Because you've got 4.5, 3.5, 3.0, you've got 01, 03, 01 mini. I'm a novice. I'm like, well, what do I use each one of these for? Do you know? Could you explain to me what they're used for?\r\n\r\n\r\nPeter Correia (05:31.245)\r\nYeah, yeah, sure.\r\n\r\n\r\nPeter Correia (05:43.682)\r\nYeah. Right.\r\n\r\n\r\nPeter Correia (05:50.776)\r\nRight.\r\n\r\n\r\nPeter Correia (05:54.498)\r\nYeah, I don't know if I could give the whole kind of background,\r\n\r\n\r\nNikolas Hulewsky (05:57.934)\r\nBecause you said you use O1 and you use 3.5 and 4.5, so just curious.\r\n\r\n\r\nPeter Correia (06:02.797)\r\nYeah, so the O1 I usually use for more technical problems. So it's like more of a technical type solution where it's like a lot of coding or I give it sort of a lot of data in the context itself and then want it to sort of churn through a lot of different options, different paths to follow to kind of get to a right solution. Usually I use sort of a 4.5. It's more of writing tool or just a thought partner where I'm trying to sort of think through different things.\r\n\r\n\r\nNikolas Hulewsky (06:05.965)\r\nHmm.\r\n\r\n\r\nNikolas Hulewsky (06:26.455)\r\nMm-hmm.\r\n\r\n\r\nPeter Correia (06:30.027)\r\nmarketing tactics, things like that. I'll just kind of ask questions and try to get a better depth of understanding.\r\n\r\n\r\nNikolas Hulewsky (06:33.792)\r\nOkay.\r\n\r\n\r\nSo is O1 better for things like uploading data and then asking it to execute certain tasks?\r\n\r\n\r\nPeter Correia (06:43.232)\r\nI think so, yeah, I would say so. It just has a lot longer to think about the actual data itself and it gets some of those tasks back.\r\n\r\n\r\nNikolas Hulewsky (06:47.639)\r\nHmm.\r\n\r\n\r\nNikolas Hulewsky (06:52.085)\r\nOkay, here's an interesting thing. Hopefully you can help me with it. I've done a number of these interviews and I took 12 of them and I batched them. now 12 of them wouldn't have normally fit in the context window, but I cleaned them up. got rid of timestamps and ums and ahs and all that stuff. And so I was able to get it, I think to 110,000 tokens or something like that. I upload it and then I asked ChatGPT and I was using 4.0, hey, analyze this data for me.\r\n\r\n\r\nPeter Correia (07:00.269)\r\nOkay.\r\n\r\n\r\nPeter Correia (07:07.349)\r\nOkay.\r\n\r\n\r\nPeter Correia (07:14.07)\r\nOkay, yep.\r\n\r\n\r\nPeter Correia (07:18.594)\r\nHmm.\r\n\r\n\r\nNikolas Hulewsky (07:21.161)\r\nwhich models are the most repeated, which ones are the highest preference, what are the different businesses that use them, just a whole suite of analytics, right? And so it spits some stuff out, but the first spit out only had like five of the 12 people that I interviewed. And then I was like, well, you're missing this person, this person, this person. yeah, sorry about that. Let me go back through and make sure. And then it spit back out.\r\n\r\n\r\nPeter Correia (07:24.689)\r\nHmm.\r\n\r\n\r\nPeter Correia (07:43.469)\r\nYeah.\r\n\r\n\r\nNikolas Hulewsky (07:46.51)\r\n10 of the 12 and I'm like, well, you're still missing this person in this. Anyway, so like I went through this iterative process with it where it was obviously missing data. And like even one of the people said, I really like grok and it didn't have it listed with that person. And I just remembered in the conversation that they had said that. And then I said, that you missed this. you're right. I'm sorry about that. Why did it miss that? Like was I using four, should I have used one instead of four, to do that type of analysis?\r\n\r\n\r\nPeter Correia (08:01.119)\r\nMm-hmm.\r\n\r\n\r\nPeter Correia (08:06.19)\r\nAlright.\r\n\r\n\r\nPeter Correia (08:15.598)\r\nSo I'd say probably, think O1 would probably be a better one for that type of analysis, again, because it will have more time to think. It's the difference between, I think it's the new models are using the test time inference. So when you actually call these models, they've done through all their training, and there's just this model that's sitting there. And you're calling it specifically. And so for 4.0, you're just calling that model directly and getting back the response. However, when you're doing test time inference,\r\n\r\n\r\nIt's going through that model and performing more processing power on that. it's doing more what they call thinking over the data. But for large context, something that I hadn't mentioned previously is Gemini. I know people have talked a whole lot about Gemini, Google's model. So that has really\r\n\r\n\r\nNikolas Hulewsky (08:53.293)\r\nMmm.\r\n\r\n\r\nNikolas Hulewsky (08:58.861)\r\nyou\r\n\r\n\r\nNikolas Hulewsky (09:03.277)\r\nNo, honestly, that's that surprised me is only two people of the 20 ish that I've interviewed so far have said that's their preferred model. And can I tell you some things about Gemini that I've learned and then you can tell me why I'm where I'm wrong or if I'm wrong? I didn't realize the context window was so big. 200,000 tokens for no, no, no, I'm sorry. A million. Sorry. A million tokens. Claude is 200,000 and then open AIS is one hundred and twenty thousand. Sorry.\r\n\r\n\r\nPeter Correia (09:07.071)\r\nMm.\r\n\r\n\r\nPeter Correia (09:11.915)\r\nOkay.\r\n\r\n\r\nPeter Correia (09:20.558)\r\nMm-hmm. So that's like, oh, OK, yeah.\r\n\r\n\r\nNikolas Hulewsky (09:32.757)\r\nI, I, but I didn't realize it was so much bigger than, than the other models, which was crazy to me. The other thing that people have said is it's fantastic. If you're trying to code or automate within the Google suite, because it just knows it so well. And so if you're trying to figure something out, it does that. But other than that, people aren't that from what I've seen, they're not using it as like their go-to model.\r\n\r\n\r\nPeter Correia (09:35.234)\r\nMm-hmm.\r\n\r\n\r\nPeter Correia (09:45.464)\r\nNo.\r\n\r\n\r\nPeter Correia (09:55.638)\r\nYeah, but I think that's definitely true. I think the large context window, I found it nice for videos. So I'll put YouTube videos and things like that that I really just don't want to necessarily remember everything or just don't have time to actually watch. I'll just chat over it. I know there's the Notebook LM tool. I don't know if you've used that or anyone's talked to you about that. OK, yeah. So it's similar to that, but you're just doing it in a more defined scope because you can control more of the parameters. Notebook LM is for someone.\r\n\r\n\r\nNikolas Hulewsky (10:09.933)\r\nHmm.\r\n\r\n\r\nNikolas Hulewsky (10:13.719)\r\nDude, yes.\r\n\r\n\r\nPeter Correia (10:23.768)\r\njust who's playing with AI or just really using it more as a productivity tool versus sort of having more ways to tinker and fine tune the specific aspects of it. So for your kind of example, I think you'd probably be able to throw all the transcripts in without having to probably do as much of that pre-processing step, but also would be curious of what those results are based on that, just because the context window is so much larger.\r\n\r\n\r\nNikolas Hulewsky (10:48.173)\r\nOkay, let me ask this. Sorry, I do want to get to your example, but I think you're probably the most knowledgeable person that I've had on here from a technical background because you worked in machine learning in particular in this space. Like I've had other people who have worked in tech, but I just think your specific experience is better than anybody I've had. So I'm very curious about this. As I've been learning about the context window, and for those who don't know what the context window is,\r\n\r\n\r\nPeter Correia (10:51.374)\r\nNo, that's all right.\r\n\r\n\r\nPeter Correia (11:00.44)\r\nMm-hmm.\r\n\r\n\r\nMm-hmm.\r\n\r\n\r\nNikolas Hulewsky (11:16.375)\r\nWhen you're asking chat GPT a question, it's been trained on this model. has all this data and all this information that it pulls on, but it doesn't have everything. It's not trained on every piece of information that's ever been out there. So an example that I give is if you're in healthcare, maybe you want it to help you understand whether or not someone has COPD, which is a diagnosis. Well, in order for the model to help you, you need to give it some context. And so you might upload, here's the American.\r\n\r\n\r\nPeter Correia (11:36.814)\r\nOkay.\r\n\r\n\r\nNikolas Hulewsky (11:44.206)\r\nHeart associations guidelines for COPD. Here's Medicare's guidelines for COPD and the criteria that's required in order to meet this diagnosis. Now that you have that information, here is a patient record. Does this patient have COPD? And so it will take the base model that it's been trained on, but now this new context that you've uploaded and then look at that patient and say, yep, these were the O2 stats. This is how much weight they've gained or lost. This is, et cetera, et cetera.\r\n\r\n\r\nyou know, a million things and they'll give you an answer. But that context window is finite for every one of these models. So for chat GPT, it's 120,000 tokens. 120,000 tokens is approximately, let's call it 300 pages. And then you've got Claude, which is 200,000 tokens. So it's a little bit bigger, almost double, but a million tokens is almost 10 times larger than chat GPT. it now turns into a 3000 page book as opposed to a 300 page book. So that's why context windows\r\n\r\n\r\nPeter Correia (12:22.68)\r\nMm-hmm.\r\n\r\n\r\nPeter Correia (12:36.782)\r\nRight.\r\n\r\n\r\nNikolas Hulewsky (12:43.775)\r\nare important because you don't necessarily want to refine these models or train these models on your own data, but you do want them have some type of context to answer the questions that you want to upload. So anyway, so that's what people mean when they're talking about context windows. Did I miss anything on the context window explanation?\r\n\r\n\r\nPeter Correia (12:47.342)\r\nYeah.\r\n\r\n\r\nPeter Correia (12:57.9)\r\nNo, yeah, I think that's good. I think also speaking on context window is thinking about the output as well. So the context that gets brought in when you think about that is also part of the output. So you may not have as robust of an output because you filled up that context with the input data. So this sort of thing is to keep in mind.\r\n\r\n\r\nNikolas Hulewsky (13:15.135)\r\nI did not know that. Hold on, hold on, hold on. Say that again. What does that mean?\r\n\r\n\r\nPeter Correia (13:19.758)\r\nYeah, so it's kind of a total, when you think about context, it's a total context, right? So if you fill out the whole context window, right, there's a possibility that you're not going to also get as diverse of an output because it's sort of a finite, it's a fixed amount of space.\r\n\r\n\r\nNikolas Hulewsky (13:35.61)\r\nI didn't know that. So it's not just about the input.\r\n\r\n\r\nPeter Correia (13:37.57)\r\nYeah, so if you put in, right, yeah, exactly. There's an output context window as well. So a of these models have shorter output context windows. So they've just now had larger and larger ones, and that's what makes them even better for coding. Because at some point, you would have to have stopped at maybe, let's say, 300 lines of code. And so that gets hard to iterate and do a lot of edits and things like that, because it can't just turn through an entire file of coding.\r\n\r\n\r\nNikolas Hulewsky (14:06.413)\r\nIs there an optimal use of the context window? If you're like, only use up to 80 % of the context window because you want that other 20 % to be used for the output or does it not matter?\r\n\r\n\r\nPeter Correia (14:15.96)\r\nSo usually the output is so much smaller than the input that it sort of doesn't matter that much. So usually you're going to sort of fill up that. Yeah, there's also, yeah, there's input and output context, Yeah.\r\n\r\n\r\nNikolas Hulewsky (14:20.503)\r\nOkay.\r\n\r\n\r\nNikolas Hulewsky (14:23.789)\r\ncrazy.\r\n\r\n\r\nNikolas Hulewsky (14:27.243)\r\nYou just freaking blew my mind, dude. my gosh. OK, so but my question on the context window in particular is, as I've read, Claude, for example, OK, larger context window than chat, GPT. However, the output may not be as precise. Because you it's a larger context window, it can't necessarily be as precise or as fast as chat, GPT. Is that true for Google as well? It's like, cool, you've got a million.\r\n\r\n\r\nPeter Correia (14:55.416)\r\nMm.\r\n\r\n\r\nNikolas Hulewsky (14:56.141)\r\ntoken context window, if you're looking for precise answers, it's not going to be able to give you that.\r\n\r\n\r\nPeter Correia (15:01.804)\r\nYeah, pretty much. mean, that's the hard thing is all the different evaluation models are really sort of, right? Everyone's sort of trying to fight for supremacy right across closed source models and open source models. And so everyone will say, right, there's sort of the classic example was the needle in the haystack kind of model where they would put some type of embedding into the context. And it would sort of prove that, I was able to find anything in this context window because I sort of hid this.\r\n\r\n\r\nparticular value in the large context window and the model was able to recall it. Well, everyone now sort of hits that benchmark. So it's no longer super valid. So everyone will sort of say that it has really good recall over a large context. But I think in practice, most people have realized it didn't. And so therefore, using, you know, you rag retrieval, retrieval, generation, I don't know if you touched on that at all, but that's really helpful when you have those really large contexts that are going to fit within\r\n\r\n\r\nNikolas Hulewsky (15:32.877)\r\nMm.\r\n\r\n\r\nNikolas Hulewsky (15:48.523)\r\nInteresting.\r\n\r\n\r\nPeter Correia (16:01.198)\r\noutside of any particular type of actual context window, you would store these documents in chunks, or it's called embeddings. And essentially you would do retrieval based on this vector search. So essentially just a similarity of your question with the actual documents, and it would retrieve subsets of documents and then embed that into the context. So if you're going through...\r\n\r\n\r\nhundreds of files, right, thousands of files, you wouldn't be able to put that in any type of context window. And so that's when people use tools like RAG to chunk out those specific portions of those documents that may be most relevant to the question and feed that into the context and then are able to then answer questions based off of that.\r\n\r\n\r\nNikolas Hulewsky (16:45.655)\r\nSo here's my question for you. Since you're very knowledgeable on this, business owners, there's kind of two functions in my mind for AI right now. If I were to break them down, one's qualitative, one's quantitative. The qualitative are like, how do I write this piece of marketing material? How do I think through this problem, this process? Like help me, you're an assistant, you're an analyst, you're somebody who's giving them advice. And then there's a quantitative aspect which...\r\n\r\n\r\nit frankly I didn't use up until recently, which is here's a spreadsheet. Help me understand my cost structure. Where am I spending that I shouldn't be spending? And that can be anywhere from a hundred lines to a thousand lines to ten thousand lines, depending on how long of a time period you're giving, etc. Which of the models are best for the quantitative side of uploading a spreadsheet and then asking for insights from that spreadsheet where it's not going to hallucinate. It's not going to do that example that I gave earlier where it's going to miss, you know, five of the seven people.\r\n\r\n\r\nPeter Correia (17:41.357)\r\nOkay.\r\n\r\n\r\nNikolas Hulewsky (17:44.941)\r\nIs there a model that's better for that than others?\r\n\r\n\r\nPeter Correia (17:49.752)\r\nSo probably not, I would say. There's probably no great model that's going to be great at inputting data and necessarily getting outputs that are not going to be any type of hallucinations involved. I think the best models and the best paradigm for doing data analysis is going to be through using AI to generate code. Because code is something that when you run code,\r\n\r\n\r\nout of 100 times, it's going to give you the same response. The inputs are going to match the outputs versus AI models. There's still always a chance that it could be hallucinating something in there. So if you're going to want something that's discrete and finite as your output, you're going to want to use AI to generate specific blocks of code to run over the data. And that's where there's a lot of tools out there to do that data analysis using AI. I Google came out with one recently for data scientists, essentially.\r\n\r\n\r\nNikolas Hulewsky (18:19.053)\r\nHmm.\r\n\r\n\r\nPeter Correia (18:42.67)\r\nYou don't need to be a data scientist to ask those questions and have the model write the code to actually go over spreadsheets and things like that.\r\n\r\n\r\nNikolas Hulewsky (18:45.933)\r\nNikolas Hulewsky (18:50.541)\r\nYeah, because like if you're a business owner and you're relying on this to make inferences or to make a decision, you know, give inferences that allow you to make a decision. You don't want to hallucinating, but what you just said makes total sense, right? Like now you're to want to use it to write the code that allows you to pull out the exact data and information to that. That's a really good insight. OK, I'm like this will be my last question on this thread. But so what would I use then? Let's say I put the data in.\r\n\r\n\r\nPeter Correia (19:00.952)\r\nRight.\r\n\r\n\r\nPeter Correia (19:05.784)\r\nMm-hmm.\r\n\r\n\r\nPeter Correia (19:09.634)\r\nBye.\r\n\r\n\r\nPeter Correia (19:14.135)\r\nYeah.\r\n\r\n\r\nNikolas Hulewsky (19:19.457)\r\nAnd I'm like, help me write code to look at this data. Where do I put the code? how does that work?\r\n\r\n\r\nPeter Correia (19:25.71)\r\nYeah, so Google Suite, I think, is pretty good because they have these things called notebooks, essentially, that let you run the code cell by cell and sort of then see the output through there. So that's sort of an easy way to start versus trying to sort of spin up your own ID or some type of process to get started to actually write certain Python code. You can do it directly in there. It's just easier to do. OK, yeah, Google Colab. Yeah, exactly.\r\n\r\n\r\nNikolas Hulewsky (19:41.143)\r\nMmm.\r\n\r\n\r\nNikolas Hulewsky (19:51.298)\r\nYeah,\r\n\r\n\r\nPeter Correia (19:57.358)\r\nI could be...\r\n\r\n\r\nOkay.\r\n\r\n\r\nPeter Correia (20:06.28)\r\nI have a little bit.\r\n\r\n\r\nNikolas Hulewsky (20:07.947)\r\nOkay, you know, I called it Zapier for like eight years until I heard some another human being called Zapier. So okay, CoLab is definitely the way to use it. my gosh. Okay, so then I would use chat.tp to say, hey, I've got this data file, I want to extract insights from it, help me write code that will extract the insights that I want. And then I go to Google CoLab. And that's where I would then input the Python script, have it run, and then it would spit out a CSV that I could then go and analyze.\r\n\r\n\r\nPeter Correia (20:10.392)\r\nThank you. Yeah.\r\n\r\n\r\nPeter Correia (20:16.088)\r\nYeah.\r\n\r\n\r\nPeter Correia (20:37.932)\r\nYep, exactly. then so recently, I think Google has come out with a tool that's now directly should be embedded in that tool to sort of help streamline that process. But again, it's hard to get, know, again, that's where you sort of ask the question, right, as someone who may not be proficient in statistics and sort of all those things, right, once you start getting into the complexity of data science, right, now you have a thought partner in those chat GPTs or those clods to sort of ask questions over, am I doing this right? Does this make sense?\r\n\r\n\r\nNikolas Hulewsky (21:03.223)\r\nRight.\r\n\r\n\r\nPeter Correia (21:07.598)\r\nWe're asking all those questions that you may ask Google, but now it's in a conversation over your own stuff.\r\n\r\n\r\nNikolas Hulewsky (21:12.513)\r\nYeah, the thing I've started doing now is like every time it gives me an output, I just take the output. And put it back into chat, tbt and I'm like this is the output it generated. And then it's like, I see the problem. I accidentally wrote this and not this and then it'll fix it. And you know, it's like this iterative process. It's never just one and done. OK, I want to see how you're using. AI in your business. I want to learn about MCPs. Please help me.\r\n\r\n\r\nPeter Correia (21:17.174)\r\nBye.\r\n\r\n\r\nPeter Correia (21:23.618)\r\nRight.\r\n\r\n\r\nPeter Correia (21:39.278)\r\nYeah, sure. So yeah, let me share over here and take a look.\r\n\r\n\r\nPeter Correia (22:07.746)\r\nWhoops.\r\n\r\n\r\nNikolas Hulewsky (22:12.887)\r\ncan edit this part out.\r\n\r\n\r\nPeter Correia (22:14.35)\r\nAll right.\r\n\r\n\r\nAll right, can you see my screen here? All right, yes. So Claude is definitely, yeah. So Claude is the tool of choice that I've been using for actually a lot of my development work as well. So one of the cool things about Claude is the ability to actually access these MCPs that a lot of folks have been talking about, which really is how I think about it is it's nothing more than using APIs essentially.\r\n\r\n\r\nNikolas Hulewsky (22:18.945)\r\nYep, I'm looking at Claude.\r\n\r\n\r\nGood morning, Pete.\r\n\r\n\r\nPeter Correia (22:46.508)\r\nthat you can interact with through a chat-like interface, which is actually sort of really nice. I've gone away from using tools like Windsurf to do development work, because using Claude to actually write scripts and actually write different functions in your own file system has been just really useful in terms of having a lot more sort of control over\r\n\r\n\r\nthe inputs and the outputs versus using some tools like cursor or windsurf where you don't have as much fine grain control over sort of something agentic actions.\r\n\r\n\r\nPeter Correia (23:27.746)\r\nSo specifically, can kind of pull up. I have a lot of MCP tools, but the ones that I've been using a lot, I'm not sure.\r\n\r\n\r\nNikolas Hulewsky (23:33.997)\r\nCan you, I'm so sorry, can you explain what MCP means?\r\n\r\n\r\nPeter Correia (23:38.38)\r\nYeah, so I don't know I have the best. I'm more of a, in this case, a tool user than someone who understands actually a whole lot behind the scenes of MCP and a model context protocol. Yeah, generally it just allows the model to call tools that it wasn't trained on or didn't have access to. So being able to do a web search, it can call a tool to do a web search.\r\n\r\n\r\nNikolas Hulewsky (23:47.863)\r\nDon't worry, nobody listening is a techie. So you just gotta be like, well, generally it's this.\r\n\r\n\r\nPeter Correia (24:07.766)\r\nRight now I have one to connect to my Notion board, but as I do some type of project management activities, I can talk to my Notion board, ask some of the activities that I want to do. It can create different cards and can move cards through my Kanban board. So I can do that all from this interface without having to actually go to Notion to do any of that.\r\n\r\n\r\nNikolas Hulewsky (24:21.131)\r\nMm-hmm.\r\n\r\n\r\nNikolas Hulewsky (24:28.641)\r\nYeah, I had somebody on last week who was showing he was demoing this, this product that he made that was text to voice. And it was running on a local server and he was showing me again. I'm way out of my depth here, but I'm just trying to explain what I saw. It's like, it's like the humans, you know, anybody who saw UFO just like trying to use their words to describe what they saw anyways. so on his screen was this, I don't know, wall of code. however, he was using Claude.\r\n\r\n\r\nPeter Correia (24:35.575)\r\nOkay.\r\n\r\n\r\nNikolas Hulewsky (24:58.251)\r\nto interface with it, where it was like, that's not exactly what I was trying to do. And then would actually change the code. And that was kind of mind blowing to see. it kind of opens up a whole new world when you're able to execute those tasks from an interface that you're already used to, like Claude, as opposed to having the, Claude, tell me what to do. Okay, cool. Now then you go to Kanban and you create the card and you create the flow. Claude creating that for you is much better and much easier.\r\n\r\n\r\nPeter Correia (24:59.758)\r\nHmm.\r\n\r\n\r\nYeah.\r\n\r\n\r\nPeter Correia (25:06.339)\r\nYeah.\r\n\r\n\r\nPeter Correia (25:27.406)\r\nRight, exactly. so now something I can do, let me see if I can use the speaking of text to voice.\r\n\r\n\r\nI thought I'd work here.\r\n\r\n\r\nOK, I'll just type it out here. So yeah, so I can just say.\r\n\r\n\r\nNikolas Hulewsky (25:40.749)\r\nYeah, no worries.\r\n\r\n\r\nPeter Correia (25:55.192)\r\nLet's say create a Python demo, but first.\r\n\r\n\r\nPeter Correia (26:12.526)\r\nSo let's first create a actual Notion card or a engineer to do this work. And then let's create the Python Hello World demo. So I can start off here. It'll ask me to go ahead and confirm these tools. So right now, I can't just hit it and let it go. I of still have to be here and watch what it's doing as it sort of works across the different file systems. So right now, it's reading Notion, going out to Notion.\r\n\r\n\r\nNikolas Hulewsky (26:22.028)\r\nYeah.\r\n\r\n\r\nPeter Correia (26:42.171)\r\nand looking to create a database page.\r\n\r\n\r\nNikolas Hulewsky (26:45.643)\r\nAnd sorry, the prompt is small. Will you read just the prompt that you put in the window?\r\n\r\n\r\nPeter Correia (26:48.622)\r\nYeah, it says, yep, let's create a Hello World Python demo. And then first, let's create a actual card on my Notion board for an engineer to tackle.\r\n\r\n\r\nNikolas Hulewsky (27:00.695)\r\nSo that simple, hey, let's create a card on my Notion board. And it knows that it just has to ask you for permissions and it starts going through that process.\r\n\r\n\r\nPeter Correia (27:04.14)\r\nYep, exactly.\r\n\r\n\r\nPeter Correia (27:10.56)\r\nYep, yeah, exactly. So it and was able to. So it's creating and running all these different tools that it has access to. So it's going out there. It's looking for what it needs to do. It's figuring out that it's not able to do that. And then it decides, OK, this is the tool that I need to actually execute that task. So it created an actual card here. And so now I can just say using my mcp.\r\n\r\n\r\nFile system tool.\r\n\r\n\r\nNikolas Hulewsky (27:51.241)\r\nin format? What did you say?\r\n\r\n\r\nPeter Correia (27:53.406)\r\nMCP tool to implement this. So I can just tell it directly to use a tool to go ahead and actually implement this Hello World example file. now it's going directly into my environment. It's on my machine, and it's creating a new path, a new example. Let's try again later. So let's see. It'll retry here.\r\n\r\n\r\nNikolas Hulewsky (28:02.765)\r\nOh, so it just knows.\r\n\r\n\r\nNikolas Hulewsky (28:31.117)\r\nAnd when did they come out with this tool or the ability for it to do this?\r\n\r\n\r\nPeter Correia (28:35.532)\r\nIt's got to be a few months back now that they came out with this.\r\n\r\n\r\nNikolas Hulewsky (28:45.889)\r\nYeah, because having the ability to be in the middle of a conversation or, you know, middle of working with it and saying like, yeah, remind, you know, create a notion reminder for me to, you know, finish this project. Instead of having to go to notion, create it yourself, et cetera, feels like a game changer.\r\n\r\n\r\nPeter Correia (29:02.11)\r\nRight, exactly. Yeah, that's what's really nice when you putting multiple tools together and able to combine them. So let's see here. I'll stop and try it again here.\r\n\r\n\r\nPeter Correia (29:20.942)\r\nSo can just edit, hit save.\r\n\r\n\r\nPeter Correia (29:28.78)\r\nAnd so has anyone also gone through any of the sort 3.7 updates around the thinking at all?\r\n\r\n\r\nNikolas Hulewsky (29:35.091)\r\nNo, I mean, everyone's just talked about how Sauna is so much better than 3.5, but they haven't. I don't know why.\r\n\r\n\r\nPeter Correia (29:38.476)\r\nYeah.\r\n\r\n\r\nPeter Correia (29:42.644)\r\nOK, yeah. So one of the things that they have done recently was they implemented, which is what OpenAI hopes to do, which is essentially a hybrid model. And so it's the models, based on the input, is deciding how long to think. So right now, it's in extended thinking, and used eight seconds to think time, which is just giving it more of that test time compute to iterate through and make a game plan.\r\n\r\n\r\nWhat has been cool and why I've also enjoyed working through MCPs more is that I'm also able to sort of control that thinking parameter a little bit more. And so you can sort of ask it to say, hey, I want you to think really hard about this problem. And so it'll actually, that time will increase as it spends more time sort of thinking through that particular problem. And so you can even sort of give it an amount of tokens you want it to think. So you can say, I want you to think,\r\n\r\n\r\nfor 5,000 thinking tokens. You can sort of watch. It doesn't always sort of behave exactly how you want with these models, but a lot of the times it'll sort of go through that process more and more in that actual thinking block.\r\n\r\n\r\nNikolas Hulewsky (30:51.853)\r\nSo are you using Sana a lot as a thought partner more than chat GPT at this point?\r\n\r\n\r\nPeter Correia (30:56.664)\r\nSo really I'm using chat GPT more as a thought partner and using Claude is more of the executor of those tasks.\r\n\r\n\r\nNikolas Hulewsky (31:04.427)\r\nAnd does this 3.7, does the sonnet update help with the execution piece?\r\n\r\n\r\nPeter Correia (31:09.486)\r\nSo I think so. I think it doesn't do well, I found, in integrating in other tools. So for example, with Windsurf, I've had, which is sort of a cursor alternative, I haven't had a whole lot of good experience with it. It just doesn't seem to be as effective. But something that I have noticed in the actual tool itself, it seems to just be a whole lot better. I do use a lot of it for sort of front-end design, so if I want to spin up a prototype of something really quickly.\r\n\r\n\r\nNikolas Hulewsky (31:20.065)\r\nMm-hmm.\r\n\r\n\r\nPeter Correia (31:38.176)\r\nIf I try to do that in Windsurf, it just doesn't look as nice and it just isn't as good. But if I'm able to do it in this actual sort of cloud desktop application, you can sort of actually use it. You can use the artifact. You can kind of see what it's looking like. You can actually sort of play around with it. And then you can just tell it using these MCP tools, hey, go in my actual directory and implement it so there's no sort of copy and pasting over.\r\n\r\n\r\nNikolas Hulewsky (32:04.909)\r\ntell me what's Claude doing right now? So like I could tell it's thinking, but it's also it's answering as as we're talking. So you asked it to go out and execute this task. What's it? What's it actually doing?\r\n\r\n\r\nPeter Correia (32:15.118)\r\nYep, so it was listing some directories to see if there was any sort of existing directories, searching for different files. And then it's continuing to search for files here. Not sure why it's running for a while. It seems like there may be a network connection. I was having some other issues with Claude this morning, so I don't know if there's something else going on. I've been struggling to perform this search. Let's see if I can do it one more time.\r\n\r\n\r\nNikolas Hulewsky (32:41.25)\r\nNo worries.\r\n\r\n\r\nWell, we're at 30 minutes anyways. I wanted to keep it short and tactical. So this was awesome. Like this was a fantastic conversations. So I may have to have you come back on and actually like go through another MCP demo because I didn't even let you get to it until way late in the conversation. So where can people come find you? I know you have a company Bulk. Is it bulkupload.info?\r\n\r\n\r\nPeter Correia (32:45.57)\r\nYep, sure.\r\n\r\n\r\nPeter Correia (32:54.706)\r\nGreat.\r\n\r\n\r\nYeah, yeah, definitely.\r\n\r\n\r\nPeter Correia (33:07.618)\r\nYeah, bulk upload.info. Yeah, that's right.\r\n\r\n\r\nNikolas Hulewsky (33:10.349)\r\nOkay, where else can people can find you Peter?\r\n\r\n\r\nPeter Correia (33:14.604)\r\nAnd then so just started getting into Twitter. So my handle is Biz Scout Guy. So as I start to looking to acquire some smaller businesses, you're to find me as I talk through that as well.\r\n\r\n\r\nNikolas Hulewsky (33:28.238)\r\nOkay, I love it. Biz Scout guy. We'll come find you. We'll come find you over there. Thanks for the conversation. Stay on for a minute while I finish this uploading.\r\n\r\n\r\nPeter Correia (33:33.602)\r\nYeah, that's awesome.\r\n\r\n\r\nYeah.",
  "metadata": {
    "duration": "",
    "date": "2025-05-08",
    "platform_source": "Google Drive"
  }
}